## [MainPage](../../index.md)/[Computer Vision](../README.md)/[Lecture](../Lecture.md)/2-3 RAW

语音识别：Youtube 转文本  
断句与标点：chatGPT 3.5  
翻译：chatGPT 4o  

# 2.3 Основные конструктивные элементы нейросетевых классификаторов изображений <br>神经网络图像分类器的基本结构要素

В настоящее время самые эффективные и прогрессивные методы классификации изображений основаны на глубоких сверточных нейронных сетях. Данную картину можно наблюдать с 2012 года. Далее мы рассмотрим наиболее знаковые, часто используемые и эффективные нейросетевые архитектуры, применяемые для классификации изображений, а также их основные конструктивные элементы и принципы построения.  
目前，最有效和先进的图像分类方法基于深度卷积神经网络。自2012年以来，这一趋势一直显而易见。接下来，我们将探讨用于图像分类的最重要、最常用和最有效的神经网络架构，以及它们的主要结构元素和构建原理。

Самым простым для понимания элементом глубоких нейросетевых архитектур, пожалуй, является конструктивный элемент, который был использован в классической архитектуре перцептрона — полносвязанный блок. Часто он применяется в качестве одного из конструктивных элементов. В глубоких нейросетевых архитектурах данные блоки представляют собой искусственные нейроны, распределённые по слоям, где каждый нейрон предшествующего слоя влияет на каждый нейрон текущего. Каждый нейрон текущего слоя, в свою очередь, влияет на каждый нейрон последующего слоя.  
深度神经网络架构中最容易理解的元素可能是全连接层，这一构造元素在经典的感知器架构中已经使用过。全连接层经常作为构造元素之一应用。在深度神经网络架构中，这些层代表人工神经元，分布在各个层中，每个前一层的神经元都会影响当前层的每个神经元。当前层的每个神经元又会影响下一层的每个神经元。

Очевидным недостатком такой конструкции является огромное число весов, возникающее при попытке построить и обучить глубокую нейросетевую архитектуру, способную качественно обрабатывать данные, такие как изображения высокого разрешения. Рассмотрим подробнее принцип работы искусственного нейрона, который является основным элементом архитектуры перцептрона.  
这种结构的明显缺点是，当试图构建和训练能够高质量处理高分辨率图像数据的深度神经网络架构时，会产生大量的权重。接下来，我们将更详细地探讨作为感知器架构基本元素的人工神经元的工作原理。

Конструктивно искусственный нейрон крайне прост и имеет естественное биологическое основание, так как в некотором приближении моделирует работу нейрона из нервной системы человека. Каждая компонента входного сигнала домножается на соответствующий ей вес, после чего к результату скалярного произведения входного вектора и вектора весов применяется функция активации, значение которой передаётся дальше в качестве результата.  
从结构上看，人工神经元非常简单，并且具有自然的生物学基础，因为它在一定程度上模拟了人类神经系统中神经元的工作原理。每个输入信号的分量都乘以相应的权重，然后将输入向量与权重向量的点积结果应用于激活函数，激活函数的值作为结果传递下去。

Веса в данном случае являются обучаемыми параметрами данного элемента модели из предыдущего слоя. Нам становится понятно место функции активации в структуре нашей модели. Каковы же задачи функции активации? В первую очередь, это приведение выходов предшествующих элементов архитектуры к определённому диапазону значений.  
在这种情况下，权重是从前一层模型中学习到的参数。这样一来，我们就可以理解激活函数在我们模型结构中的位置。激活函数的任务是什么呢？首先，它将前一层架构的输出转换到特定的数值范围内。

Отметим, что от конкретных свойств конкретной функции активации в определённом месте нейросетевой архитектуры зачастую зависит как само функционирование модели, так и возможность её обучения. Помимо дифференцируемости, которая позволяет эффективно применять градиентный спуск для решения задачи нахождения оптимальных весов, определённый интерес при исследовании функции активации представляют и другие свойства.  
值得注意的是，具体激活函数在神经网络架构中的具体位置决定了模型的功能和其学习的可能性。除了可微性（这允许有效地应用梯度下降法来找到最优权重），激活函数的其他属性也很有趣。

Нелинейность активации позволяет уже с помощью двухуровневой архитектуры построить универсальный аппроксиматор функций и таким образом существенно повысить мощность рассматриваемого инструмента при решении той или иной задачи. Разумеется, по сравнению с линейными тождественными функциями активации ещё одним важным свойством функции активации является ограниченная область значений, что позволяет избежать проблем в процессе оптимизации.  
非线性激活函数允许通过两层架构构建通用函数逼近器，从而大大提高在解决特定任务时的工具功效。显然，相较于线性恒等激活函数，激活函数的另一个重要属性是有限的数值范围，这可以避免优化过程中的问题。

Помимо вышеупомянутого условия при оптимизации важную роль играет возможность аппроксимировать тождественную функцию в окрестности 0. Если данное условие не выполнено, могут возникнуть проблемы при начальной инициализации весов. Рассмотрим далее распространённые функции активации, их основные свойства и область применения.  
除了优化过程中提到的条件外，逼近0邻域的恒等函数的能力也很重要。如果不满足这一条件，权重的初始值设置可能会出现问题。接下来我们将讨论常见的激活函数及其主要属性和应用领域。

Классическим вариантом функции активации является сигмоид. Он же логистическая регрессия — гладкая монотонно возрастающая нелинейная функция, которая усиливает слабые сигналы и насыщается от сильных. Используется логистическая регрессия в основном в качестве активации для полносвязанных слоёв, в том числе и на выходах глубоких моделей.  
经典的激活函数是Sigmoid，也称为逻辑回归——一种平滑、单调递增的非线性函数，它能增强弱信号，并在强信号时饱和。逻辑回归主要用于全连接层的激活，包括深度模型的输出层。

Несомненными плюсами данной функции являются ограниченная область значений в интервале от 0 до единицы, монотонность и простой аналитический вид производной. Однако нельзя не отметить, что данная функция фактически гладко аппроксимирует ступеньку, что делает результат её применения удобным для использования в качестве выхода в глубокой модели при решении задачи классификации, но одновременно ведёт к проблеме затухающих градиентов при использовании внутри глубоких свёрточных нейронных сетей.  
这种函数的优点包括在0到1范围内的有限值域、单调性和简单的解析导数形式。然而，不得不提的是，该函数实际上平滑地逼近阶跃函数，这使得其在解决分类问题的深度模型输出时非常方便，但在深度卷积编码器中使用时会导致梯度消失问题。

Причиной этому служит быстрое насыщение при отдалении от начала координат. Гиперболический тангенс по своим свойствам напоминает сигмоид, он также склонен усиливать слабые сигналы при всей простоте аналитического вида производной и ограниченной области значений. Он также склонен к быстрому насыщению при больших по модулю входных значениях и влечёт затухание градиентов при использовании в качестве активации в глубоких свёрточных нейронных сетях.  
这是因为在远离坐标原点时，函数迅速饱和。双曲正切函数的性质类似于Sigmoid，它也倾向于增强弱信号，具有简单的解析导数和有限的值域。然而，在输入值绝对值较大时，它也会迅速饱和，并在深度卷积编码器中使用时导致梯度消失问题。

Однако следует отметить безусловное преимущество гиперболического тангенса над сигмоидой — центрированность относительно 0, что позволяет градиентам быть как положительными, так и отрицательными в процессе оптимизации весов, что позволяет избежать нежелательного зигзагообразного эффекта.  
然而，双曲正切函数相对于Sigmoid有一个无可争议的优势，即其以0为中心，这使得在优化权重的过程中梯度既可以为正也可以为负，从而避免了不必要的锯齿效应。

Для использования в качестве функции активации в глубоких свёрточных нейронных сетях часто используется та или иная модификация активационной функции ReLU.  
在深度卷积神经网络中，常用的激活函数是ReLU的各种变体。