## [MainPage](../../index.md)/[Computer Vision](../README.md)/[Lecture](../Lecture.md)/2-3 RAW

语音识别：Youtube 转文本  
断句与标点：chatGPT 3.5  
翻译：  

В настоящее время самые эффективные и прогрессивные методы классификации изображений основаны на глубоких сверточных нейронных сетях. Данную картину можно наблюдать с 2012 года. Далее мы рассмотрим наиболее знаковые, часто используемые и эффективные нейросетевые архитектуры, применяемые для классификации изображений, а также их основные конструктивные элементы и принципы построения.

Самым простым для понимания элементом глубоких нейросетевых архитектур, пожалуй, является конструктивный элемент, который был использован в классической архитектуре перцептрона — полносвязанный блок. Часто он применяется в качестве одного из конструктивных элементов. В глубоких нейросетевых архитектурах данные блоки представляют собой искусственные нейроны, распределённые по слоям, где каждый нейрон предшествующего слоя влияет на каждый нейрон текущего. Каждый нейрон текущего слоя, в свою очередь, влияет на каждый нейрон последующего слоя.

Очевидным недостатком такой конструкции является огромное число весов, возникающее при попытке построить и обучить глубокую нейросетевую архитектуру, способную качественно обрабатывать данные, такие как изображения высокого разрешения. Рассмотрим подробнее принцип работы искусственного нейрона, который является основным элементом архитектуры перцептрона.

Конструктивно искусственный нейрон крайне прост и имеет естественное биологическое основание, так как в некотором приближении моделирует работу нейрона из нервной системы человека. Каждая компонента входного сигнала домножается на соответствующий ей вес, после чего к результату скалярного произведения входного вектора и вектора весов применяется функция активации, значение которой передаётся дальше в качестве результата.

Веса в данном случае являются обучаемыми параметрами данного элемента модели из предыдущего слоя. Нам становится понятно место функции активации в структуре нашей модели. Каковы же задачи функции активации? В первую очередь, это приведение выходов предшествующих элементов архитектуры к определённому диапазону значений.

Отметим, что от конкретных свойств конкретной функции активации в определённом месте нейросетевой архитектуры зачастую зависит как само функционирование модели, так и возможность её обучения. Помимо дифференцируемости, которая позволяет эффективно применять градиентный спуск для решения задачи нахождения оптимальных весов, определённый интерес при исследовании функции активации представляют и другие свойства.

Нелинейность активации позволяет уже с помощью двухуровневой архитектуры построить универсальный аппроксиматор функций и таким образом существенно повысить мощность рассматриваемого инструмента при решении той или иной задачи. Разумеется, по сравнению с линейными тождественными функциями активации ещё одним важным свойством функции активации является ограниченная область значений, что позволяет избежать проблем в процессе оптимизации.

Помимо вышеупомянутого условия при оптимизации важную роль играет возможность аппроксимировать тождественную функцию в окрестности 0. Если данное условие не выполнено, могут возникнуть проблемы при начальной инициализации весов. Рассмотрим далее распространённые функции активации, их основные свойства и область применения.

Классическим вариантом функции активации является сигмоид. Он же логистическая регрессия — гладкая монотонно возрастающая нелинейная функция, которая усиливает слабые сигналы и насыщается от сильных. Используется логистическая регрессия в основном в качестве активации для полносвязанных слоёв, в том числе и на выходах глубоких моделей.

Несомненными плюсами данной функции являются ограниченная область значений в интервале от 0 до единицы, монотонность и простой аналитический вид производной. Однако нельзя не отметить, что данная функция фактически гладко аппроксимирует ступеньку, что делает результат её применения удобным для использования в качестве выхода в глубокой модели при решении задачи классификации, но одновременно ведёт к проблеме затухающих градиентов при использовании внутри глубоких свёрточных нейронных сетей.

Причиной этому служит быстрое насыщение при отдалении от начала координат. Гиперболический тангенс по своим свойствам напоминает сигмоид, он также склонен усиливать слабые сигналы при всей простоте аналитического вида производной и ограниченной области значений. Он также склонен к быстрому насыщению при больших по модулю входных значениях и влечёт затухание градиентов при использовании в качестве активации в глубоких свёрточных нейронных сетях.

Однако следует отметить безусловное преимущество гиперболического тангенса над сигмоидой — центрированность относительно 0, что позволяет градиентам быть как положительными, так и отрицательными в процессе оптимизации весов, что позволяет избежать нежелательного зигзагообразного эффекта.

Для использования в качестве функции активации в глубоких свёрточных нейронных сетях часто используется

 та или иная модификация активационной функции ReLU.