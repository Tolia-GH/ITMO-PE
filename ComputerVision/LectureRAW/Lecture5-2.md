## [MainPage](../../index.md)/[Computer Vision](../README.md)/[Lecture](../Lecture.md)/5-2 RAW

语音识别：Youtube 转文本
断句与标点：chatGPT 4o
翻译：chatGPT 4o

В общем говоря, если рассматривать сегментирование изображений как кластеризацию пикселов, то для решения задачи сегментирования нам подойдут хорошо известные классические алгоритмы кластеризации. Поэтому мы не можем пройти мимо K-means. Алгоритм является неконтролируемым, используется для отделения сегментов друг от друга или от фона. В частности, он группирует и разделяет данные на $k$ кластеров или частей на основе $k$ центроидов. Алгоритм используется, когда есть не размеченные данные, то есть данные без определенных категорий и групп. То есть данный алгоритм не использует никакой априорной информации, помимо количества кластеров. Цель состоит в том, чтобы найти определенные группы на основе некоторого сходства данных с количеством групп, которое обозначено $k$. Цель кластеризации K-means является минимизация суммы квадратов расстояний между всеми точками и центром кластера.  
总的来说，如果将图像分割视为像素的聚类，那么解决分割问题可以采用众所周知的经典聚类算法。因此，我们不能忽略K-means算法。该算法是非监督的，用于将分割对象彼此分离或从背景中分离出来。具体来说，它基于k个质心将数据分组和划分为k个簇或部分。该算法在没有标注数据的情况下使用，即数据没有预定的类别和组。因此，该算法除了簇的数量外，不使用任何先验信息。目标是基于数据的某种相似性找到指定数量的组，数量为k。K-means聚类的目标是最小化所有点到簇中心的平方距离总和。

Шаги в алгоритме K-means:  
K-means算法的步骤：

1. Сначала выбирается количество кластеров, то есть $k$.  
   首先选择簇的数量，即 $k$。
2. Далее выбираются случайным образом $k$ точек, которые являются центроидами. Они не обязательно принадлежат набору данных.  
   然后随机选择 $k$ 个点作为质心。它们不一定属于数据集。
3. Далее происходит назначение каждой точки из классифицируемого множества к ближайшему центроиду. Таким образом формируется $k$ кластеров.  
   接下来，将分类集合中的每个点分配到最近的质心，从而形成 $k$ 个簇。
4. Далее вычисляется новый центроид для каждого кластера.  
   然后为每个簇计算新的质心。
5. После этого каждая точка из классифицируемого множества назначается новому ближайшему центроиду.  
   之后，将分类集合中的每个点重新分配到新的最近质心。
6. Если точки были переназначены к центроиду, который и так был за ними закреплен, или же достигнуто другое условие остановки, то алгоритм завершается. В противном случае происходит итеративный пересчет центроидов с переназначением точек.  
   如果点已被分配到原本就属于它们的质心，或达到其他停止条件，则算法结束。否则，继续迭代计算质心并重新分配点。

Возникает вопрос, как выбрать оптимальное значение $k$. Для определенного класса алгоритмов кластеризации, в частности для K-means, есть параметр, обычно называемый $k$, который определяет количество кластеров для обнаружения. Ну, собственно говоря, как и в нашем случае. Если говорить именно о K-means, то правильный выбор $k$ часто неоднозначен, с интерпретациями, которые зависят от формы и масштаба распределения точек в наборе данных и желаемого разрешения кластеризации для пользователя. Кроме того, увеличение $k$ без штрафа всегда будет уменьшать количество ошибок в результирующей кластеризации до крайнего случая — ноль ошибок, если каждая точка данных рассматривается как свой собственный кластер, то есть когда $k$ равно количеству точек данных. Тогда интуитивно оптимальный выбор $k$ обеспечит баланс между максимальным сжатием данных с использованием одного кластера и максимальной точностью, назначая каждую точку данных своему кластеру. Если подходящее значение $k$ не очевидно из предыдущего значения свойств набора данных, его нужно каким-то образом выбрать.  
如何选择最优的k值呢？对于某类聚类算法，特别是 K-means，有一个参数通常称为k，它定义了要检测的簇的数量。在我们的情况下也是如此。具体来说，对于 K-means，正确选择k值通常是模棱两可的，解释取决于数据集中点的分布形状和规模以及用户所需的聚类分辨率。此外，增加k值不会有惩罚，始终会减少结果聚类中的错误数量，直到极端情况——零错误，即每个数据点被视为自己的簇，这时 k 值等于数据点的数量。直观上，最优的 k 值选择将平衡一个簇的最大压缩和分配每个数据点到其自身簇的最大精度。如果从先前的数据集属性中不能明显看出合适的k值，则需要某种方法来选择它。

Есть несколько категорий методов для принятия этого решения. Метод локтя — один из таких. Основная идея методов разделения, таких как кластеризация $k$-средних, состоит в том, чтобы определить кластеры таким образом, чтобы общая вариация внутри кластера, или, другими словами, общая сумма квадратов внутри кластера, была минимизирована. Таким образом достигается компактность кластеризации и уменьшается, то есть производится оптимизация данного параметра. Метод локтя преследует именно такую цель.  
有几类方法可以做出这个决定。肘部法则是其中之一。分割方法（如K-means聚类）的基本思想是确定簇，以便簇内总变异或簇内平方和最小化，从而实现聚类的紧凑性并优化该参数。肘部法则正是为了达到这个目标。

Рассмотрим подробнее шаги по выбору оптимального количества кластеров:  
详细考虑选择最佳簇数的步骤：

1. Вычисляется кластеризация методом K-means для различных значений $k$, варьируя $k$ от 1 до 10.  
   使用 K-means 算法对不同的 $k$ 值进行聚类，$k$ 值从 1 到 10。
2. Для каждого $k$ вычисляется общая сумма квадратов внутри кластера.  
   对于每个 $k$ 值，计算簇内总平方和。
3. Далее строится кривая зависимости общей суммы квадратов от количества кластеров. Расположение изгиба на данной кривой обычно рассматривается как показатель соответствующего количества кластеров.  
   然后绘制总平方和与簇数的关系曲线。曲线的弯曲处通常被认为是合适的簇数的指示。

Но есть подвох: несмотря на все преимущества, K-means иногда терпит неудачу из-за случайного выбора центроидов. И такая ситуация называется случайной ловушкой инициализации. Чтобы решить эту проблему, у нас есть процедура инициализации для K-means, которая называется K-means++. Она предлагает выбор начальной инициализации для K-means. K-means++ выбирает точку случайным образом и это первый центроид. Затем выбирается следующая точка на основе вероятностей, которые зависят от расстояния до первой точки: чем дальше точка, тем более вероятно её выбор. Затем получается второй центроид. Далее процесс повторяется с учетом того, что вероятность каждой точки основана на расстоянии до ближайшего центроида к этой точке. Настоящее время это приводит к накладным расходам при инициализации алгоритма, но снижает вероятность неправильной инициализации, приводящей к плохому результату кластеризации.  
然而，尽管有诸多优点，K-means有时因随机选择质心而失败，这种情况称为初始化的随机陷阱。为了解决这个问题，我们有一个称为K-means++的初始化程序。它建议为K-means选择初始值。K-means++随机选择一个点作为第一个质心。然后根据到第一个点的距离概率选择下一个点：距离越远的点被选择的概率越大。这样得到第二个质心。然后重复该过程，每个点的概率基于其到最近质心的距离。这会在算法初始化时带来开销，但减少了错误初始化导致聚类结果不佳的概率。

Главное отличие алгоритмов AESDTA и K-means заключается в том, что на стадии инициализации алгоритма AESDTA происходит распределение пикселов, в то время как для алгоритма K-means происходит распределение значений математических ожиданий. Поэтому ниже будет рассмотрен результат применения алгоритма AESDTA. Данный алгоритм использует минимальное спектральное расстояние для определения соответствующего кластера для каждого пикселя. Процесс начинается с назначения случайного или приближенного среднего значения кластера и повторяется до тех пор, пока это значение не достигнет величины среднего для каждого кластера исходных данных. Начальные средние значения кластеров распределяются равномерно вдоль центрального вектора спектрального пространства. В течение первой итерации кластеризации пространство равномерно разбивается на области, центром каждой из которых являются средние значения кластеров. Пиксель анализируется с левого верхнего угла изображения к правому нижнему. Вычисляется спектральное расстояние между пикселями и средним значением кластера. Пикселы назначаются в тот кластер, где расстояние минимально. После итерации рассчитываются реальные средние значения спектральных признаков полученных кластеров, так как их средние значения меняются в зависимости от преобладающих яркостей, попавших в них пикселов. Затем выполняется вторая итерация, в процессе которой повторяется кластеризация с новыми средними значениями и рассчитываются границы кластеров. После этого определяются новые средние значения и выполняется новая итерация. В процессе второй итерации снова определяются минимальное спектральное расстояние между точками и новыми средними значениями кластеров, по окончании которой пиксели будут перераспределены. Такие пересчеты повторяются до тех пор, пока все пиксели с заданной вероятностью или порогом сходимости не попадут в какой-либо кластер. Отметим, что возможна ситуация, когда распределение значений яркости на снимке не фиксируются в каком-либо кластере, поэтому ограничивающим фактором здесь будет являться заданное число итераций.  
AESDTA和K-means算法的主要区别在于，AESDTA算法的初始化阶段分配的是像素，而K-means算法分配的是数学期望值。因此，下面将考虑AESDTA算法的应用结果。该算法使用最小光谱距离来确定每个像素对应的簇。过程从分配随机或近似的簇平均值开始，直到达到原始数据的每个簇的平均值。初始簇的平均值均匀分布在光谱空间的中央向量上。第一次迭代过程中，空间均匀划分为区域，每个区域的中心是簇的平均值。像素从图像的左上角到右下角进行分析。计算像素与簇平均值之间的光谱距离。像素被分配到距离最小的簇。迭代后，计算所得簇的实际光谱特征平均值，因为它们的平均值根据进入的像素亮度变化。然后执行第二次迭代，使用新的平均值重新进行聚类，并计算簇边界。接着确定新的平均值并执行新的迭代。在第二次迭代过程中，重新确定点与新簇平均值之间的最小光谱距离，最终重新分配像素。这种重新计算持续进行，直到所有像素以指定的概率或收敛阈值进入某个簇。注意，可能存在亮度值分布未固定在任何簇中的情况，因此迭代次数是限制因素。

Перейдем к очередному алгоритму кластеризации ForAll от слова сочетания «формальный элемент». Это метод кластеризации, основанный на идее объединения в один кластер объектов в областях их наибольшего сгущения. В данном случае цель кластеризации стандартная — разбить выборку на такое заранее неизвестное число кластеров, чтобы сумма расстояний от объектов кластеров до центров кластеров была минимальной по всем кластерам. Таким образом, задача выглядит как выделение группы максимально близких друг другу объектов, которые по гипотезе схожести и будут образовывать наши кластеры. Таким образом, задача состоит в минимизации функционала качества, который представлен на слайде: первое суммирование ведется по всем кластерам выборки, второе — по всем объектам $x$, принадлежащим текущему кластеру. $W_{k}$ — это центр текущего кластера соответственно. При этом $ρ(x, y)$ — это расстояние между двумя объектами.  
接下来是另一种聚类算法ForAll，名称来源于“形式元素”组合。它是一种基于最大密度区域中对象合并为一簇的聚类方法。在这种情况下，聚类的目标是标准的——将样本划分为未知数量的簇，以使所有簇中对象到簇中心的距离总和最小化。因此，任务是找出彼此最接近的对象组，这些对象根据相似性假设将形成我们的簇。任务是最小化质量函数，如下图所示：第一个求和遍历样本的所有簇，第二个求和遍历当前簇中属于的所有对象x。$W_k$ 分别是当前簇的中心。 $ρ(x, y)$ 是两个对象之间的距离。

Отметим необходимые условия для использования алгоритмов:  
使用算法的必要条件：

- Во-первых, выполнение гипотезы компактности, предполагающей, что близкие друг другу объекты с большой вероятностью принадлежат к одному кластеру.  
  首先，紧凑性假设需要满足，即彼此接近的对象很可能属于同一簇。
- Во-вторых, наличие линейного или метрического пространства кластеризуемых объектов.  
  其次，存在线性或度量空间的聚类对象。

Итак, на вход алгоритму подается кластеризуемая выборка, которая может быть задана признаковыми описаниями объектов, это, как мы уже говорили, линейное пространство, либо матрица парных расстояний между объектами. При этом в реальных задачах на больших данных зачастую хранение всех данных невозможно и бессмысленно, но необходимые данные собираются в процессе кластеризации. Параметр $r$ — радиус поиска локальных сгущений — его можно задавать как из априорных соображений, то есть знания диаметра кластеров, так и настраивать скользящим контролем. Также в модификации данного подхода возможно введение параметра $k$, то есть количество кластеров. На выходе мы получаем кластеризацию на заранее неизвестное число кластеров. На выходе алгоритма в классической его реализации будут центры кластеров и области их действия. Также при необходимости алгоритм может быть адаптирован для случайного числа кластеров и для постоянного.  
因此，算法输入是待聚类样本，可以是对象特征描述，线性空间或对象之间的配对距离矩阵。在实际的大数据任务中，通常无法存储所有数据，但必要数据在聚类过程中收集。参数r是局部密集区域的搜索半径，可以根据先验知识设置，即簇直径，或者通过滑动控制进行调整。在经典实现中，算法输出为簇的中心及其作用区域。根据需要，算法还可以适应不同数量的簇和固定数量的簇。

Сначала выделим объекты, находящиеся в областях их максимального сгущения, радиус поиска областей $r$ будет настраиваемым. Далее объединяем в один кластер объекты с пересекающимися областями максимального сгущения. Далее для каждого из полученных кластеров определяем его центр, так как $W_{k}$, о которых мы говорили ранее. Переходим к следующей итерации, то есть к следующему циклу сходимости и повторяем шаги 1 и 2. Центр каждого из кластеров определяется как среднее геометрическое всех объектов кластера. После того как центр пересчитан, мы можем перестроить кластеры. Этот процесс повторяется до тех пор, пока центр кластера не станет фиксированным. В качестве примера можно привести задачи кластеризации изображений по признаковому описанию либо задачи поиска в пространстве многомерных объектов в пространстве признаков. Также метод может быть адаптирован для разного рода формальных и физических признаков. Сложность алгоритма составляет $O(n^{2})$, если не накладывать дополнительных условий на априорное описание кластеров.  
首先，识别在最大密度区域中的对象，搜索半径r可调整。然后将具有重叠最大密度区域的对象合并为一簇。接着为每个簇确定其中心，如前文所述的\(W_k\)。进入下一个迭代，即下一个收敛周期并重复步骤1和2。每个簇的中心确定为簇中所有对象的几何平均值。重新计算中心后，我们可以重构簇。这个过程重复进行，直到簇中心固定。以图像特征描述聚类或多维对象特征空间搜索任务为例。该方法也可以适应各种形式和物理特征。算法复杂度为\(O(n^2)\)，如果不对簇的先验描述施加附加条件。

Кроме описанных выше методов, есть и другие. Например, алгоритмы разделения, такие как алгоритм BIRCH, который использует деревья кластеров, или DBSCAN, который базируется на плотности данных. Как мы видим, выбор алгоритма зависит от конкретных условий задачи, а также имеющихся данных и потребностей.  
除了上述方法，还有其他方法。例如，BIRCH算法使用聚类树，DBSCAN算法基于数据密度。选择算法取决于具体任务条件以及可用的数据和需求。

Основные вопросы, возникающие в процессе кластеризации изображений и выборов алгоритмов:  
图像聚类和算法选择过程中出现的主要问题：

- Как мы понимаем, что кластеризация выполнена хорошо? Существует несколько метрик для оценки качества кластеризации. Например, индекс Дэвиса-Боулдена или индекс Силуэта. Эти метрики позволяют оценить компактность кластеров, их разделимость и т.д.  
  如何判断聚类效果好？有几个评估聚类质量的指标。例如，戴维斯-鲍尔丁指数或轮廓指数。这些指标可以评估簇的紧凑性和可分离性等。
- Что делать, если данные имеют неявную структуру? В таких случаях можно использовать иерархические методы или методы на основе плотности данных.  
  如果数据具有隐含结构该怎么办？在这种情况下，可以使用层次方法或基于数据密度的方法。
- Как выбрать количество кластеров? Как уже упоминалось выше, метод локтя может помочь в этом. Также можно использовать и другие методы, такие как анализ основной компоненты (PCA) или другие методы снижения размерности.  
  如何选择簇的数量？如上所述，肘部法则可以帮助解决这个问题。还可以使用其他方法，如主成分分析（PCA）或其他降维方法。

Надеюсь, этот обзор был полезен для вас, и теперь вы лучше понимаете, как применять алгоритмы кластеризации к изображениям.  
希望这个概述对你有所帮助，现在你对如何将聚类算法应用于图像有了更好的理解。