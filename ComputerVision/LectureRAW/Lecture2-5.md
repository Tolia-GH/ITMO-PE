## [MainPage](../../index.md)/[Computer Vision](../README.md)/[Lecture2-5](./Lecture2-x.md)/RAW

语音识别：Youtube 转文本  
断句与标点：  
翻译：  

ни для кого не секрет что самый распространенный метод обучения нейронных сетей это градиентный спуск в общем виде правилу обновления весов модели представлена на слайде теперь параметр не отвечает за величину шага его принято называть скоростью обучения на самом деле представленном виде данный метод практически не применим в современных задачах глубокого обучения в силу целого ряда причин к примеру для того чтобы сделать всего один шаг по методу градиентного спуска то есть сделать всего одно изменение параметров сети необходимо подать на вход сити последовательно абсолютно весь набор обучающих данных затем для каждого объекта обучающих данных вычислить ошибку и рассчитать необходимую коррекцию коэффициенту в сети но данную коррекцию не применять и уже после подачи всех данных рассчитать шуму в корректировке каждого коэффициента сети то есть сумму градиентов и произвести коррекцию коэффициентов на один шаг очевидно что при большом наборе обучающих данных такой алгоритм будет работать крайне медленно для того чтобы исправить упомянутую ранее проблему применяется концепция стохастического градиентного спуска подобная идея ускорение алгоритма заключается в использовании только одного элемента либо некоторые подвыборки для подсчета нового приближения весов при этом процесс обучения глубоких архитектур на реальных наборах данных ускоряется настолько что становится возможным из прочих достоинств этого метода можно отметить также простоту реализации применимость для задач с большими данными то есть иногда можно получить решение даже не обработав всю выборку но нельзя и не и сказать пару слов о недостатках среди которых отсутствие универсального набора эвристик для выбора элементов таким образом данные стратегии лучше строить для конкретной задачи отдельно в рамках обучения классификатора обычно используется выбор элементов из различных классов рассмотрим далее некоторые усовершенствования стохастического градиентного спуска первым из них является метод импульсов данный алгоритм запоминает изменения дельта w на каждой итерации и определяет следующее изменение в виде линейной комбинации градиента и предыдущего извинения нетрудно догадаться что данная оптимизация уходит корнями физику в отличие от классического стохастического градиентного спуска метод пытается сохранить продвижение в том же направлении предотвращая колебания теперь рассмотрим одни из самых часто применимых и демонстрирующих в общем случае лучше эмпирические результаты усовершенствованные методы стохастического градиентного спуска первый из них рут рискуя пропадаешь и основная идеи данного подхода заключается в накоплении истории обновлений параметров для уменьшения реакции весов по наиболее часто изменяющихся при обновлении направлениям причем наибольшее внимание в истории обновлений удивляется именно последний им изменением параметров таким образом уменьшается значимость общего фона в данного и подчеркивается наиболее редки вследствие чего значимые направления обновления адептов mounts развивает идею рут минску я про погибших еще дальше помимо накопления и сокращения движения наиболее частым направлением изменения градиента посредством сокращения 2 момента путем накопления первого момента стимулируется движение в направлении глобального улучшения данное сочетание стратегий дает очень хороший результат и по умолчанию рекомендуется использовать адаптив момент для оптимизации весов глубоких нейросетей если конечно нет никаких известных причин против 