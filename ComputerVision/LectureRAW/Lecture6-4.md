## [MainPage](../../index.md)/[Computer Vision](../README.md)/[Lecture](../Lecture.md)/6-4 RAW

语音识别：Youtube 转文本  
断句与标点：chatGPT 4o  
翻译：chatGPT 4o  

Ну и в заключение рассмотрим еще один тип генеративных моделей. Он носит название вариационный автокодировщик. Но для начала поговорим про автокодировщики в целом.

Итак, автокодировщик — это относительно простая архитектура, которая последовательно производит сжатие исходных изображений в некоторое латентное пространство и обратное преобразование из этого латентного пространства в исходное изображение. При этом различия должны быть минимальны. За это отвечают две составляющие архитектуры автокодировщика: кодировщик и декодер. Кодировщик производит отображение исходной информации в сжатое пространство, а декодер производит обратное преобразование. Данная схема отображена на слайде.

Теперь поговорим непосредственно о вариационных автокодировщиках. Вариационные автокодировщики представляют собой генеративный алгоритм, который добавляет дополнительное ограничение для кодирования входных данных, а именно то, что скрытые представления нормализуются. Вариационные автокодировщики способны сжимать данные, как автокодировщики, и синтезировать данные, подобно GAN. Однако, в то время как GAN генерирует детализированные изображения, созданные с помощью вариационных автокодировщиков, как правило, бывают более размытыми.

Итак, для начала немного установим нашу терминологию. Так же как и в обычных автокодировщиках, у нас имеется скрытое вероятностное пространство Z, соответствующее случайной величине. Данная случайная величина имеет определенное распределение, которое как-нибудь фиксировано. Возьмем это распределение нормальным с нулевым математическим ожиданием и единичной дисперсией. Далее мы хотим иметь декодер, который из элемента этого пространства и еще какого-нибудь параметра восстанавливает нам некоторые изображения из исходного распределения, то есть из пространства, которое подавалось на вход кодировщику.

Область определения кодировщика и вся суть работы архитектуры автокодировщика хорошо отображает иллюстрация, представленная на данном слайде. Сначала рассмотрим левую часть. С помощью кодировщика мы генерируем латентное представление, которое мы хотим, чтобы было максимально похоже на нормальное распределение с нулевым математическим ожиданием и единичной дисперсией. Для этого мы их приближаем, используя дивергенцию Кульбака-Лейблера. Далее нам остается сэмплировать из этого распределения некоторый образец, декодировать его и набросить L2-лосс для того, чтобы максимизировать схожесть на исходные данные, которые подавались на вход кодировщику.

Но есть одна проблема. Дело в том, что операция сэмплирования не пропускает градиенты, а для обучения нам нужно применять градиентный спуск. Для этого используется трюк реконструкции. Как это работает? Давайте смотреть на правую часть картинки. Мы опять пропускаем исходные данные через кодировщик, получая некоторое латентное пространство. При этом мы также стремимся, чтобы распределение элементов этого пространства было похоже на нормальное с нулевым математическим ожиданием и единичной дисперсией. Также для этого используем дивергенцию Кульбака-Лейблера. Но сэмплирование мы производим отдельно из нормального распределения и добавляем его как дополнительный множитель в линейной комбинации полученного элемента из латентного пространства и его самого с коэффициентом единица. После этого уже мы применяем декодер и с помощью L2 стремимся сделать выход похожим на вход.

Рассмотрим еще подробнее эту идею. Мы передаем декодеру не сам элемент латентного пространства, а линейную комбинацию изначально полученного этого элемента с еще одним членом, который представляет собой произведение сэмплированного элемента из нормального распределения с нулевым математическим ожиданием и единичной дисперсией и его самого. Что это нам дает? Дело в том, что сэмплированный элемент при подсчете градиентов исчезает. Таким образом, градиенты спокойно проходят и не ломаются об операцию сэмплирования. Собственно говоря, в этом и заключается ключевая идея, которая позволяет обучать вариационные автокодировщики.

На этом лекция завершается. Спасибо за внимание.