## [MainPage](../../index.md)/[Computer Vision](../README.md)/[Lecture](../Lecture.md)/6-4 RAW

语音识别：Youtube 转文本  
断句与标点：chatGPT 4o  
翻译：chatGPT 4o  

Ну и в заключение рассмотрим еще один тип генеративных моделей. Он носит название вариационный автокодировщик. Но для начала поговорим про автокодировщики в целом.  
那么，最后我们再来看一种生成模型。它被称为变分自编码器。不过首先，我们来聊一下自编码器。

Итак, автокодировщик — это относительно простая архитектура, которая последовательно производит сжатие исходных изображений в некоторое латентное пространство и обратное преобразование из этого латентного пространства в исходное изображение. При этом различия должны быть минимальны. За это отвечают две составляющие архитектуры автокодировщика: кодировщик и декодер. Кодировщик производит отображение исходной информации в сжатое пространство, а декодер производит обратное преобразование. Данная схема отображена на слайде.  
自编码器是一种相对简单的结构，它依次将原始图像压缩到某个潜在空间，并从这个潜在空间反向重建回原始图像。在此过程中，差异应尽可能最小。负责这一过程的自编码器架构包含两个部分：编码器和解码器。编码器将原始信息映射到压缩空间，而解码器则进行反向转换。此方案在幻灯片中有所展示。

Теперь поговорим непосредственно о вариационных автокодировщиках. Вариационные автокодировщики представляют собой генеративный алгоритм, который добавляет дополнительное ограничение для кодирования входных данных, а именно то, что скрытые представления нормализуются. Вариационные автокодировщики способны сжимать данные, как автокодировщики, и синтезировать данные, подобно GAN. Однако, в то время как GAN генерирует детализированные изображения, созданные с помощью вариационных автокодировщиков, как правило, бывают более размытыми.  
现在我们直接讨论变分自编码器。变分自编码器是一种生成算法，它为输入数据的编码添加了额外的限制，即隐含表示必须规范化。变分自编码器可以像自编码器一样压缩数据，也可以像生成对抗网络（GAN）一样生成数据。然而，与生成对抗网络生成细致的图像不同，变分自编码器生成的图像通常较为模糊。

Итак, для начала немного установим нашу терминологию. Так же как и в обычных автокодировщиках, у нас имеется скрытое вероятностное пространство Z, соответствующее случайной величине. Данная случайная величина имеет определенное распределение, которое как-нибудь фиксировано. Возьмем это распределение нормальным с нулевым математическим ожиданием и единичной дисперсией. Далее мы хотим иметь декодер, который из элемента этого пространства и еще какого-нибудь параметра восстанавливает нам некоторые изображения из исходного распределения, то есть из пространства, которое подавалось на вход кодировщику.  
首先，我们来定义一些术语。与普通自编码器一样，我们有一个隐含的概率空间Z，对应于随机变量。该随机变量具有某种固定的分布。我们假设这个分布是零均值和单位方差的正态分布。接下来，我们希望有一个解码器，可以从该空间的一个元素和某个参数恢复出一些来自原始分布的图像，即输入编码器的空间。

Область определения кодировщика и вся суть работы архитектуры автокодировщика хорошо отображает иллюстрация, представленная на данном слайде. Сначала рассмотрим левую часть. С помощью кодировщика мы генерируем латентное представление, которое мы хотим, чтобы было максимально похоже на нормальное распределение с нулевым математическим ожиданием и единичной дисперсией. Для этого мы их приближаем, используя дивергенцию Кульбака-Лейблера. Далее нам остается сэмплировать из этого распределения некоторый образец, декодировать его и набросить L2-лосс для того, чтобы максимизировать схожесть на исходные данные, которые подавались на вход кодировщику.  
自编码器的定义域及其工作原理在幻灯片中有详细说明。首先来看左边部分。通过编码器，我们生成潜在表示，我们希望它尽可能接近零均值和单位方差的正态分布。为此，我们使用库尔贝克-莱布勒（Kullback-Leibler）散度来逼近它们。接下来，我们从该分布中采样一个样本，解码它，并添加L2损失，以最大限度地增加与输入编码器的原始数据的相似性。

Но есть одна проблема. Дело в том, что операция сэмплирования не пропускает градиенты, а для обучения нам нужно применять градиентный спуск. Для этого используется трюк реконструкции. Как это работает? Давайте смотреть на правую часть картинки. Мы опять пропускаем исходные данные через кодировщик, получая некоторое латентное пространство. При этом мы также стремимся, чтобы распределение элементов этого пространства было похоже на нормальное с нулевым математическим ожиданием и единичной дисперсией. Также для этого используем дивергенцию Кульбака-Лейблера. Но сэмплирование мы производим отдельно из нормального распределения и добавляем его как дополнительный множитель в линейной комбинации полученного элемента из латентного пространства и его самого с коэффициентом единица. После этого уже мы применяем декодер и с помощью L2 стремимся сделать выход похожим на вход.  
但有一个问题。采样操作不传递梯度，而我们需要应用梯度下降进行训练。为此，使用了重参数化技巧。它是如何工作的？我们来看右边部分。我们再次通过编码器传递原始数据，获得一个潜在空间。同时，我们也努力使该空间的元素分布接近零均值和单位方差的正态分布。我们仍然使用库尔贝克-莱布勒散度来实现这一点。但采样是从正态分布中单独进行的，并将其作为线性组合的额外因子添加到从潜在空间获得的元素和其本身中，系数为1。之后，我们应用解码器，并通过L2损失使输出尽可能接近输入。

Рассмотрим еще подробнее эту идею. Мы передаем декодеру не сам элемент латентного пространства, а линейную комбинацию изначально полученного этого элемента с еще одним членом, который представляет собой произведение сэмплированного элемента из нормального распределения с нулевым математическим ожиданием и единичной дисперсией и его самого. Что это нам дает? Дело в том, что сэмплированный элемент при подсчете градиентов исчезает. Таким образом, градиенты спокойно проходят и не ломаются об операцию сэмплирования. Собственно говоря, в этом и заключается ключевая идея, которая позволяет обучать вариационные автокодировщики.  
更详细地看这一想法。我们传递给解码器的不是潜在空间的元素本身，而是最初获得的该元素与从零均值和单位方差正态分布中采样的元素的乘积的线性组合。这有什么好处？采样元素在计算梯度时会消失。因此，梯度可以顺利传递，不受采样操作的影响。实际上，这就是允许训练变分自编码器的关键思想。

На этом лекция завершается. Спасибо за внимание.  
本次讲座到此结束。感谢大家的关注。