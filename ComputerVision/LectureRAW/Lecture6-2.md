## [MainPage](../../index.md)/[Computer Vision](../README.md)/[Lecture](../Lecture.md)/6-2 RAW

语音识别：Youtube 转文本  
断句与标点：chatGPT 4o  
翻译：chatGPT 4o  

Перейдём теперь, пожалуй, к самому популярному семейству генеративных моделей в компьютерном зрении, то есть к генеративно-состязательным сетям. Как уже говорили ранее, мы будем называть их ГАНами (от английской аббревиатуры), потому что на данный момент это наиболее общее и употребимое название. Отметим также, что ГАН представляет собой скорее некоторую схему обучения нейросетей, нежели конкретные архитектуры.  
现在，让我们来讨论计算机视觉中最流行的一类生成模型，即生成对抗网络。我们之前已经提到过，我们将其称为GAN（根据其英文缩写），因为目前这是最常见和普遍使用的名称。需要注意的是，GAN更像是一种神经网络的训练方法，而不是具体的架构。

Рассмотрим подробнее принцип работы ГАН. Одна нейронная сеть, называемая генератором, генерирует новые экземпляры данных, а другая, называемая дискриминатором, оценивает их на подлинность. То есть дискриминатор решает, относится ли каждый экземпляр данных, которые он рассматривает, к набору тренировочных данных или же нет. Генератор создаёт новые изображения, которые он передаёт дискриминатору, делая это в надежде, что они будут приняты за подлинные, хотя на самом деле являются поддельными. Цель генератора состоит в том, чтобы научиться воспроизводить изображения как можно более похожие на тренировочный набор, чтобы обмануть дискриминатор. Цель дискриминатора, соответственно, — определять любые изображения, которые были сгенерированы генератором, то есть не являются изображениями из тренировочного набора.  
我们详细了解一下GAN的工作原理。一个称为生成器的神经网络生成新的数据样本，而另一个称为判别器的神经网络则评估这些样本的真实性。判别器的任务是判断每个数据样本是来自训练数据集还是生成器生成的。生成器创建新的图像，并将它们传递给判别器，试图让判别器将这些图像视为真实图像，尽管它们实际上是假的。生成器的目标是学习生成尽可能接近训练集的图像，以欺骗判别器。相应地，判别器的目标是识别由生成器生成的图像，即那些不属于训练集的图像。

Итак, рассмотрим подробнее шаги, которые проходит ГАН. Генератор получает на вход рандомное число, а возвращает изображение. Это сгенерированное изображение подаётся в дискриминатор наряду с потоком изображений, взятых из фактического набора данных. Дискриминатор принимает как реальные, так и поддельные изображения и возвращает вероятность, значение которой находится в интервале от нуля до единицы. При этом 1 представляет собой реакцию на изображение, которое было принято подлинным, а 0 — на то, которое было определено как фальшивое.  
现在，让我们详细了解GAN的工作步骤。生成器接收一个随机数作为输入，然后输出一张图像。生成的图像与从实际数据集获取的一批图像一起传递给判别器。判别器接受真实图像和生成的图像，并返回一个概率值，该值在0到1之间。这里，1表示图像被认为是真实的，而0表示图像被认为是假的。

Таким образом, появляется двойной цикл обратной связи: дискриминатор находится в цикле с достоверными изображениями, а генератор — в цикле вместе с дискриминатором. Обычно в качестве дискриминатора в компьютерном зрении используется свёрточная сеть, которая может классифицировать изображения, подаваемые на неё, как бинарный классификатор, распознающий изображения как реальные или поддельные. Генератор в некотором смысле представляет собой обратную свёрточную сеть: стандартный свёрточный классификатор принимает изображение и уменьшает его разрешение, чтобы получить вероятность, тогда как генератор принимает вектор случайного шума и преобразует его в изображение.  
因此，出现了一个双重反馈循环：判别器与真实图像一起循环，而生成器则与判别器一起循环。通常，在计算机视觉中，判别器使用卷积神经网络，可以将输入的图像分类为真实或伪造的二元分类器。生成器在某种程度上可以看作是一个反卷积神经网络：标准卷积分类器接收图像并通过下采样得到一个概率值，而生成器则接收一个随机噪声向量，并将其转换为图像。

Таким образом, дискриминатор отсеивает данные с помощью методов понижения дискретизации, таких как макс-пуллинг, а генератор генерирует новые данные. Рассмотрим подробнее процесс обучения ГАНов. Для этого рассмотрим эскиз, представленный на слайде, на котором изображена последовательность распределений, а точнее взаимное соотношение распределений данных из обучающего множества, данных, порождаемых генератором, и разделяющего критерия. Итак, генеративно-состязательные сети или ГАНы обучаются путём одновременного обновления дискриминирующего распределения (здесь оно обозначено синей пульсирующей линией) так, чтобы дискриминатор мог различать объекты из распределения тренировочного набора (обозначенного чёрной пунктирной линией, пунктир в виде точек) и данных из распределения генератора (обозначены зелёной сплошной линией).  
因此，判别器通过如最大池化等下采样方法来过滤数据，而生成器则生成新的数据。让我们更详细地了解GAN的训练过程。为此，我们来看一下幻灯片上的草图，其中显示了数据分布的序列，具体来说，是训练集数据的分布、生成器生成的数据的分布以及分割准则之间的相互关系。即，生成对抗网络或GAN通过同时更新判别器分布（这里用蓝色波动线表示），使判别器能够区分训练集分布（用黑色虚线表示，虚线为点状）和生成器分布（用绿色实线表示）中的对象来进行训练。

Нижняя горизонтальная линия представляет собой область, из которой составлена выборка. В нашем случае сэмплирование произведено равномерно. Горизонтальная линия на дне является частью области, в которую происходит отображение с помощью генератора. Таким образом, происходит генерация из равномерного в неравномерное распределение, как показано на рисунке. Как видим, отображение производит сжатие в областях с высокой плотностью и расширение в областях с низкой. Далее, в последовательности фрагментов мы видим, как путём обновления градиента происходит подстройка отображения, которое осуществляет генератор. Области фрагментов распределены согласно распределению, издаваемому обучающей выборкой.  
下方的水平线表示采样区域。在我们的例子中，采样是均匀进行的。底部的水平线是生成器映射到的区域的一部分。因此，生成器将均匀分布生成到非均匀分布中，如图所示。可以看出，生成在高密度区域会收缩，而在低密度区域会扩展。接下来，在一系列片段中，我们看到，通过梯度更新，生成器的映射进行调整。片段区域根据训练集生成的分布进行分布。

Теперь давайте попробуем формализовать только что рассмотренные идеи и принципы в виде явного описания алгоритма обучения ГАНов. Итак, нетрудно догадаться, что наш алгоритм будет итеративным. На каждой итерации у нас фигурируют две фазы: в рамках первой фазы мы обучаем дискриминатор при фиксированном генераторе, а в рамках второй фазы мы обучаем генератор при фиксированном дискриминаторе. Отметим, что как правило, обучение дискриминатора также осуществляется путём повторения нескольких операций, и это обусловлено тем, что полностью оптимизировать дискриминатор вычислительно невыгодно, и на ограниченных наборах он может переобучиться. Также отметим, что на практике не всегда удобно использовать представленные на слайде формулы. К примеру, в начале обучения дискриминатор может не учитывать объекты с высокой уверенностью в классификации. Это обусловлено тем, что генератор ещё настроен плохо и генерируемые данные сильно отличаются от обучающего множества. В этом случае логарифм разности единицы и результата дискриминации сгенерированных данных стагнирует. Чтобы избежать этого, можно вместо минимизации данного логарифма максимизировать логарифм результата дискриминации сгенерированных данных.  
现在，让我们尝试将刚刚讨论的想法和原则形式化，显式描述GAN的训练算法。显然，我们的算法是迭代的。在每次迭代中有两个阶段：在第一个阶段，我们在固定生成器的情况下训练判别器；在第二个阶段，我们在固定判别器的情况下训练生成器。需要注意的是，通常判别器的训练是通过多次重复操作来进行的，因为完全优化判别器在计算上是不可取的，并且在有限的样本上它可能会过拟合。还需要指出的是，在实践中，使用幻灯片上所示的公式并不总是方便的。例如，在训练初期，判别器可能不会考虑高置信度分类的对象。这是因为生成器在此时还没有很好地调整，生成的数据与训练集差异较大。在这种情况下，生成数据的判别结果与1的对数差会停滞。为了避免这种情况，可以在训练初期最大化生成数据的判别结果的对数，而不是最小化该对数的差值。

Нельзя не сказать о некоторых проблемах, присущих ГАНам. Дело в том, что большинство ГАНов подвержены схлопыванию мод распределения, или, как это часто называют на английский манер, mode collapse. То есть генератор "коллапсирует" и выдает ограниченное количество разных образцов, моды склеиваются и отсутствует разнообразие, которое подразумевалось обучающей выборкой. Также для ГАНов зачастую характерна проблема стабильности обучения: параметры модели дестабилизируются и не сходятся. Помимо этого, также актуальной проблемой является исчезающий градиент: дискриминатор становится слишком сильным, а градиент генератора исчезает и обучение не происходит. Кроме этого, существует и проблема запутывания, то есть выявления корреляций в признаках, не связанных с признаками из реального мира. ГАНам свойственна высокая чувствительность к гиперпараметрам в процессе обучения.  
不得不提到的是，GAN存在一些问题。大多数GAN会遇到模式崩溃的问题，或者如常说的那样，mode collapse。即生成器“崩溃”，并输出有限数量的不同样本，模式合并，缺乏训练集所期望的多样性。此外，GAN通常还存在训练稳定性问题：模型参数不稳定并且不收敛。此外，还有梯度消失的问题：判别器变得过于强大，生成器的梯度消失，导致训练无法进行。还有过拟合问题，即发现与真实世界特征无关的特征之间的相关性。GAN对训练过程中的超参数非常敏感。

Вообще, все эти проблемы остаются до конца нерешёнными, так как общего подхода к решению большинства из них нет и в принципе быть не может. Но приуменьшить страдания можно, используя нормализацию данных, модификацию функции ошибки (к примеру, то, о чём мы говорили, рассматривая модернизацию алгоритма обучения, а точнее замену функции ошибки на начальной стадии обучения). Также хорошо себя показывает зачастую сэмплирование из многомерного нормального распределения вместо равномерного, использование нормализованных слоёв. Также хорошей практикой является использование дополнительного штрафа за несоответствие метки классификации, если таковая присутствует в дата-сети, то есть добавляется дополнительный компонент функции ошибки для дискриминатора.  
总体而言，这些问题至今尚未完全解决，因为没有普遍的方法解决其中的大多数问题。但是，可以通过使用数据归一化、修改损失函数（例如，之前讨论过的训练算法的改进，即在训练初期更改损失函数）来缓解这些问题。通常，使用多维正态分布而不是均匀分布进行采样、使用归一化层也是有效的。另一种好的做法是在数据集中存在分类标签时，使用额外的惩罚机制，以便在判别器的损失函数中添加额外的惩罚项。

Далее мы рассмотрим подробнее ключевые проблемы ГАНов. Начнём с mode collapse, так как это довольно часто встречающаяся проблема, довольно сложная в решении и до конца нерешённая. О ней часто говорят, но, тем не менее, в процессе обучения генератор может прийти к состоянию, при котором он будет всегда выдавать ограниченный набор выходов, то есть моды склеиваются. При этом пространство, в котором распределены сгенерированные изображения, окажется существенно меньше, чем пространство исходных изображений. Главная причина этого в том, что генератор обучается обманывать дискриминатор, а не воспроизводить исходное распределение. Это как раз-таки заложено в самой природе ГАНов: если генератор начинает каждый раз выдавать похожий выход, который является максимально правдоподобным для текущего дискриминатора, то зависимость от распределения начальных данных падает, и, следовательно, градиент генератора стремится к нулю. Лучшей стратегией для дискриминатора будет улучшение детектирования этого конкретного изображения, так на следующих итерациях наиболее вероятно, что генератор придёт к другому изображению, хорошо обманывающему текущий дискриминатор. Дискриминатор будет учиться отличать конкретно это новое изображение. Этот процесс не будет сходиться, и количество представленных мод не будет расти, поэтому приблизиться к исходному распределению не удастся. На текущий момент mode collapse является одной из главных проблем ГАНов, и эффективное решение для неё ищется до сих пор.  
接下来我们将详细讨论GAN的关键问题。首先是mode collapse，这是一个非常常见的问题，难以解决且尚未完全解决。尽管经常提到这个问题，但在训练过程中，生成器可能会陷入一种状态，在这种状态下，它每次都生成有限数量的输出，即模式合并。此时生成的图像分布空间将显著小于原始图像的分布空间。主要原因是生成器学习的是如何欺骗判别器，而不是再现原始分布。这正是GAN的本质：如果生成器每次生成的输出相似，并且对于当前的判别器来说是最可信的，那么对原始数据分布的依赖性就会下降，因此生成器的梯度趋向于零。判别器的最佳策略是改进对这个特定图像的检测，因此在下一次迭代中，生成器可能会产生另一个新的图像，能够很好地欺骗当前的判别器。判别器将学习区分这个新的图像。这个过程将不会收敛，所呈现的模式数量也不会增加，因此无法接近原始分布。目前，mode collapse是GAN的主要问题之一，寻找有效的解决方案仍在进行中。

Среди самых популярных решений можно отметить архитектуры Wasserstein GAN, которая основана на использовании метрики Вассерштайна внутри функции ошибки, что позволяет дискриминатору быстрее обучаться выявлять повторяющиеся выходы, на которых стабилизируется генератор. И WGAN-GP, где для генератора используется функция потерь, которая зависит не только от того, как текущий дискриминатор оценивает выходы генератора, но и от выходов будущих версий дискриминатора.  
最受欢迎的解决方案之一是Wasserstein GAN架构，它基于在损失函数中使用Wasserstein度量，使得判别器能够更快地学习识别生成器生成的重复输出，并且生成器可以更稳定地生成样本。WGAN-GP则是在生成器的损失函数中不仅考虑当前判别器的评估结果，还考虑未来版本判别器的输出结果。

Рассмотрим ещё одну важную проблему, которая возникает при обучении ГАНов, — это стабильность процесса обучения. Дело в том, что задача обучения дискриминатора и генератора в общем смысле не является задачей поиска локального или глобального минимума функции, а является задачей поиска точки равновесия двух игроков. В теории игр эта точка называется точкой равновесия Нэша, в которой оба игрока больше не получают выгоды, хотя следуют оптимальной стратегии.  
另一个重要问题是GAN的训练稳定性。判别器和生成器的训练任务在一般意义上并不是寻找函数的局部或全局最小值，而是寻找两个玩家的平衡点。在博弈论中，这个点被称为纳什均衡点，在这个点上，两个玩家都无法通过改变策略获得额外的好处。

Рассмотрим задачу поиска этой точки на игрушечном примере, где генератор хочет максимизировать произведение x и y, а дискриминатор — минимизировать. Будем обновлять параметры x и y на основе градиентного спуска по формулам, которые представлены на слайде. Таким образом, если изобразить на графике поведение x и y, а также произведение x и y, то станет ясно, что они не сойдутся, амплитуда их движения будет только увеличиваться. В оригинальной статье про ГАН используется дивергенция Кульбака-Лейблера, которая также представлена на слайде, и трудно не заметить, что при наличии x, в которых Q от x будет равна нулю, весь интеграл разойдётся, и мы получим проблему расходимости интеграла при обучении. Данная проблема аналогична ситуации, которую мы рассматривали на нашем игрушечном примере. Но следует отметить, что существует ряд подходов, которые позволяют бороться с данной проблемой. Во-первых, это регуляризация, к примеру, добавление шума на входы дискриминатора и настройка гиперпараметров дискриминатора. Также следует отметить, что в Wasserstein GAN в качестве функции дивергенции используют метрику Вассерштайна, которая в большинстве случаев решает проблему расходимости интеграла.  
让我们通过一个简单的例子来解释这个问题，其中生成器希望最大化x和y的乘积，而判别器希望最小化。我们将根据梯度下降的公式更新x和y的参数，如幻灯片所示。因此，如果在图上描绘x和y的行为，以及x和y的乘积，很明显，它们不会收敛，振幅会不断增加。在最初的GAN论文中，使用的是KL散度，如幻灯片所示，很难不注意到，当存在x使得Q(x)等于零时，整个积分将发散，从而导致训练过程中积分发散的问题。这个问题与我们在简单例子中讨论的问题相似。但是，有一些方法可以解决这个问题。首先是正则化，例如，在判别器输入上添加噪声并调整判别器的超参数。另外，在Wasserstein GAN中，使用Wasserstein度量作为散度函数，这在大多数情况下可以解决积分发散的问题。