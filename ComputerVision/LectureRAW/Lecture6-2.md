## [MainPage](../../index.md)/[Computer Vision](../README.md)/[Lecture](../Lecture.md)/6-2 RAW

语音识别：Youtube 转文本
断句与标点：
翻译：

перейдем теперь пожалуй к самому популярному семейству генеративных моделей в компьютерном зрение то есть генеративно состязательным сетям как уже говорили ранее мы будем называть их ганами от английская аббревиатура потому что на данный момент это наиболее общее употребим а и название отметим также что gun представляет собой скорее некоторую схему обучения нейросетей нежели конкретные архитектуры рассмотрим подробнее принцип работы gun 1 нейронная сеть называемое генератором генерирует новые экземпляры данных а другая называемое соответственно дискриминатора оценивает их на подлинность то есть дискриминатор решает относится ли каждый экземпляр данных которые он рассматривает к набору тренировочных данных или же нет генератор создают новые изображения которое он передает дискриминатор у и делает это в надежде что они будут приняты за подлинные хотя на самом деле являются поддельными цель генератора состоит в том чтобы научиться воспроизводить изображение как можно более похожее на тренировочный набор для того чтобы обмануть дискриминатор цель дискриминатора соответственно определять любые изображения которые были сгенерированы генератором то есть не являются изображениями из тренировочного набора итак рассмотрим подробнее шаги которые проходит gun генератор получает на вход рандомное число а возвращает изображение это сгенерированное изображение подается в дискриминатор наряду с потоком изображений взятых из фактического набора данных дискриминатор принимает как реальные так и поддельные изображения и возвращает вероятности все числа от нуля до единицы причем и 900 представляет собой реакцию на изображение которое было принято подлинным 0 зато которое было определено как фальшивая таким образом появляется двойной цикл обратной связи дискриминатор находится в цикле с достоверными изображениями а генератор находится в цикле вместе с дискриминатора обычно в качестве дискриминатора в компьютерному зрение используется свёрточная сеть которая может классифицировать изображение подаваемые на нее так бинарный классификатор распознающие изображения как реальные или поддельные генератор в некотором смысле представляет собой обратную свёрточная сеть хотя стандартный свёртыш или классификатор принимает изображения и уменьшает его разрешение чтобы получить вероятность в то время как генератор принимает вектор случайного шума и преобразует его в изображение таким образом дискриминатора отсеивает данные с помощью методов понижения дискретизации таких например как макс pulling a generator генерируют новые данные рассмотрим подробнее процесс обучения оганов для этого рассмотрим эскиз представленной на слайде на котором изображена последовательность распределение а точнее взаимного соотношения распределения данных из обучающего множества данных порождаемых генератором и разделяющего критерия и так генеративно состязательные сеть или ганы обучается путем одновременного обновления дискриминирующего распределения здесь она обозначена синей публике ровной линией так чтобы дискриминатор мог различать объекты из распределения тренировочного набора который обозначен черной пунктирной линией пунктир виде точек и данных из распределения генератора здесь они обозначены зеленой сплошной линией нижняя горизонтальная линия представляет собой область из которой составлена выборка в нашем случае сэмплирование произведено равномерно горизонтальная линия на дней является частью области в которую происходит отображение с помощью генератора таким образом происходит генерация из равномерного в неравномерное распределение как показано на рисунке как видим отображение производит сжатия в областях с высокой плотностью и расширения в областях с миской далее в последовательности фрагментов мы видим как путем обновления градиента происходит подстройка отображение которое осуществляет generator is simply ruim и области фрагменты распределенные согласно распределению издаваемому обучающей выборкой теперь давайте попробуем формализовать только что рассмотренные идеи и принципы виде явного описание алгоритма обучение гамов и так нетрудно догадаться что наш алгоритм будет итеративно мм и на каждой итерации у нас фигурирует две фазы в рамках первой фазы мы обучаем дискриминатор при фиксированном генераторе в рамках второй фазы мы обучаем генератор при фиксированном дискриминаторы отметим что как правило обучение дискриминатора также осуществляется путем повторения нескольких операций и это обусловлено тем что полностью оптимизировать дискриминатор вычислительно невыгодно и на ограниченных наборах он может перри обучиться также отметим что на практике не всегда удобно использовать представленные на слайде формулы к примеру в начале обучения дискриминатор может не учитывать объекты с высокой уверенностью в классификации это обусловлено тем что генератор еще настроен плохо и генерируемые данные сильно отличаются от обучающего множества в этом случае логарифм разности единицы и результата дискриминации сгенерированных данных стагнирует чтобы избежать этого можно вместо минимизации данного логарифма максимизировать логарифм результата дискриминации сгенерированных данных нельзя не сказать о некоторых проблемах присущих ганнам дело в том что большинство кранов подвержена схлопывания мод распределение или как это часто называют на английский манер мод collapse то есть генератор коллапсирует или выдает ограниченное количество разных образцов то есть моды склеиваются и отсутствует разнообразия которое подразумевалось обучающей выборке также для гамов зачастую характерно проблема стабильности обучения то есть параметры модели дестабилизируется и не сходится помимо этого также актуальной проблемой является исчезающий градиент то есть дискриминатор становится слишком сильным а градиент генератора исчезает и обучение не происходит помимо этого существует и проблема запутывания то есть выявление корреляции в признаках не связанных с признаками из реального мира также га нам свойственна высокая чувствительность гипер параметрам процессе обучения вообще все эти проблемы остаются до конца нерешенными так как общего подхода к решению большинства из них нет и в принципе быть не может но при уменьшить страдания можно используя нормализацию данных модификацию функция ошибки к примеру то о чем мы говорили рассматривая модернизацию алгоритма обучение а точнее замены функция ошибки на начальной стадии обучения также хорошо себя показывает зачастую сэмплирование из многомерного нормального распределения вместо равномерного использования нормализации ных слоев также хорошей практикой является использование дополнительного штрафа за и соответствия метки классификации если такая присутствует в dota сети то есть добавляется дополнительный компонент функция ошибки для дискриминатора далее мы рассмотрим подробнее ключевые проблемы данов начнем с мод collapse так как это довольно часто встречающаяся проблема довольно сложное в решении для конца нерешенная они часто говорят но тем ни менее в процессе обучения генератор может прийти к состоянию при котором он будет всегда выдавать ограниченный набор выходов то есть моды склеиваются при этом пространство в котором распределены и сгенерированные изображения окажется существенно меньше чем пространство исходных изображений главная причина этого в том что генератор обучается обманывать дискриминатор они воспроизводить исходное распределение ну это как раз таки заложено самой природой данов если генератор начинает каждый раз выдавать похожий выход который является максимально правдоподобным для текущего дискриминатора то зависимость от распределения начальных данных падает и следовательно градиент генератора стремится к нулю лучшей стратегии для дискриминатора будет улучшение детектирования этого конкретного изображения так на следующих итерациях наиболее вероятно что генератор придет к другому изображению хорошо обманывающий текущей дискриминатор дискриминатор будет учиться отличать конкретно это новое изображение этот процесс не будет сходиться и количество представленных мод не будет расти поэтому приблизиться к исходному распределению не удастся на текущий момент мод коллапс является одной из главных проблем gun и эффективное решение который ищется до сих пор среди самых популярных решений можно отметить архитектуры вася штайн gun которая основана на использовании метрики восер штайна внутри функции ошибки что позволяет дискриминатор у быстрее обучаться выявлять повторяющиеся выходы на которых стабилизируется генератор и юган или on-road gun где для генератора используется функция потерь которая зависит не только от того как текущий дискриминатор оценивает выхода генератора но и от выходов будущих версий дискриминатора рассмотрим еще одну важную проблему которая возникает при обучении gun of это стабильность процесса обучения дело в том то что за отдача обучения дискриминатора и генератора в общем смысле не является задачей поиска вокального или глобального минимума функции а является задачей поиска точки равновесия двух игроков в теории игр эта точка называется точкой равновесия нэша в которой оба игрока больше не получают выгоды хотя следует оптимальной стратегии рассмотрим задачу поиска этой точки на игрушечном примере где генератор хочет максимизировать произведение икса и игрека дискриминатор минимизировать будем обновлять параметры x и y на основе градиентного спуска по формулам которые представлены на слайде таким образом если изобразить на графике поведения икса и игрека и произведение икса и игрека то станет ясно что они не сойдутся амплитуда и движение будет только увеличиваться в региональный статье про gun используется дивергенция кульбака лай блэра которая также представлена на слайде и трудно заметить что при наличии иксов в которых у от x будет равна нулю весь интеграл разойдется и мы получим проблему расходимости интеграла при обучении данная проблема аналогична ситуация которую мы рассматривали на нашем игрушечном примере но следует отметить что существует ряд подходов которые позволяют бороться с данной проблемой во первых это регуляризация к примеру всякое добавление шума входом дискриминатора и настройка параметров дискриминатора гипер параметров также следует отметить что во серфингом в качестве функции дивергенции используют метрику в лосяш тайна которая в большинстве случаев решают проблему расходимости интеграла 