{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1aa382ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The eigenvalues of A: [8. 2.]\n",
      "A is positive definite\n",
      "A is symmetric\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1.41421356, 1.41421356]), 0.0)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "def f(x):  # Define the objective function\n",
    "    return 5*x[0]**2 + 6*x[0]*x[1] + 5*x[1]**2 - 8*math.sqrt(2)*x[0] - 8*math.sqrt(2)*x[1]\n",
    "\n",
    "A = np.array(([5, 3], [3, 5]), dtype=float)\n",
    "b = np.array([8*math.sqrt(2.), 8*math.sqrt(2.)])\n",
    "\n",
    "eigs = np.linalg.eigvals(A)\n",
    "print(\"The eigenvalues of A:\", eigs)\n",
    "\n",
    "if (np.all(eigs>0)):\n",
    "    print(\"A is positive definite\")\n",
    "elif (np.all(eigs>=0)):\n",
    "    print(\"A is positive semi-definite\")\n",
    "else:\n",
    "    print(\"A is negative definite\")\n",
    "\n",
    "if (A.T==A).all()==True: print(\"A is symmetric\")\n",
    "\n",
    "def linear_CG(x, A, b, epsilon):\n",
    "    res = A.dot(x) - b  # Initialize the residual\n",
    "    delta = -res  # Initialize the descent direction\n",
    "\n",
    "    while True:\n",
    "\n",
    "        if np.linalg.norm(res) <= epsilon:\n",
    "            return x, f(x)  # Return the minimizer x* and the function value f(x*)\n",
    "\n",
    "        D = A.dot(delta)\n",
    "        beta = -(res.dot(delta)) / (delta.dot(D))  # Line (11) in the algorithm\n",
    "        x = x + beta * delta  # Generate the new iterate\n",
    "\n",
    "        res = A.dot(x) - b  # generate the new residual\n",
    "        chi = res.dot(D) / (delta.dot(D))  # Line (14) in the algorithm\n",
    "        delta = chi * delta - res  # Generate the new descent direction\n",
    "\n",
    "\n",
    "linear_CG(np.array([0, 0]), A, b, 10**-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dd8a694a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.70710678, 0.70710678]), -8.000000000000002)\n"
     ]
    }
   ],
   "source": [
    "from autograd import grad\n",
    "import autograd.numpy as np\n",
    "import math\n",
    "from scipy.optimize import line_search\n",
    "NORM = np.linalg.norm\n",
    "\n",
    "def func(x): # Objective function\n",
    "    return 5*x[0]**2 + 6*x[0]*x[1] + 5*x[1]**2 - 8*math.sqrt(2)*x[0] - 8*math.sqrt(2)*x[1]\n",
    "\n",
    "Df = grad(func) # Gradient of the objective function\n",
    "\n",
    "def Fletcher_Reeves(Xj, tol, alpha_1, alpha_2):\n",
    "    x1 = [Xj[0]]\n",
    "    x2 = [Xj[1]]\n",
    "    D = Df(Xj)\n",
    "    delta = -D # Initialize the descent direction\n",
    "    \n",
    "    while True:\n",
    "        start_point = Xj # Start point for step length selection \n",
    "        beta = line_search(f=func, myfprime=Df, xk=start_point, pk=delta, c1=alpha_1, c2=alpha_2)[0] # Selecting the step length\n",
    "        if beta!=None:\n",
    "            X = Xj+ beta*delta #Newly updated experimental point\n",
    "        \n",
    "        if NORM(Df(X)) < tol:\n",
    "            x1 += [X[0], ]\n",
    "            x2 += [X[1], ]\n",
    "            return X, func(X) # Return the results\n",
    "        else:\n",
    "            Xj = X\n",
    "            d = D # Gradient at the preceding experimental point\n",
    "            D = Df(Xj) # Gradient at the current experimental point\n",
    "            chi = NORM(D)**2/NORM(d)**2 # Line (16) of the Fletcher-Reeves algorithm\n",
    "            delta = -D + chi*delta # Newly updated descent direction\n",
    "            x1 += [Xj[0], ]\n",
    "            x2 += [Xj[1], ]\n",
    "            \n",
    "print(Fletcher_Reeves(np.array([-1.7, -3.2]), 10**-5, 10**-4, 0.38))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c7fd24b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.70710678, 0.70710678]), -8.0)\n"
     ]
    }
   ],
   "source": [
    "from autograd import grad\n",
    "import autograd.numpy as np\n",
    "import math\n",
    "from scipy.optimize import line_search\n",
    "NORM = np.linalg.norm\n",
    "\n",
    "def func(x): # Objective function\n",
    "    return 5*x[0]**2 + 6*x[0]*x[1] + 5*x[1]**2 - 8*math.sqrt(2)*x[0] - 8*math.sqrt(2)*x[1]\n",
    "\n",
    "Df = grad(func) # Gradient of the objective function\n",
    "\n",
    "def Polak_Ribiere(Xj, tol, alpha_1, alpha_2):\n",
    "    x1 = [Xj[0]]\n",
    "    x2 = [Xj[1]]\n",
    "    D = Df(Xj)\n",
    "    delta = -D # Initialize the descent direction\n",
    "    \n",
    "    while True:\n",
    "        start_point = Xj # Start point for step length selection \n",
    "        beta = line_search(f=func, myfprime=Df, xk=start_point, pk=delta, c1=alpha_1, c2=alpha_2)[0] # Selecting the step length\n",
    "        if beta!=None:\n",
    "            X = Xj+ beta*delta # Newly updated experimental point \n",
    "        \n",
    "        if NORM(Df(X)) < tol:\n",
    "            x1 += [X[0], ]\n",
    "            x2 += [X[1], ]\n",
    "            return X, func(X) # Return the results\n",
    "        else:\n",
    "            Xj = X\n",
    "            d = D # Gradient of the preceding experimental point\n",
    "            D = Df(Xj) # Gradient of the current experimental point\n",
    "            chi = (D-d).dot(D)/NORM(d)**2 \n",
    "            chi = max(0, chi) # Line (16) of the Polak-Ribiere Algorithm\n",
    "            delta = -D + chi*delta # Newly updated direction\n",
    "            x1 += [Xj[0], ]\n",
    "            x2 += [Xj[1], ]\n",
    "            \n",
    "print(Polak_Ribiere(np.array([-1.7, -3.2]), 10**-6, 10**-4, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "921ecac1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.70710678, 0.70710678]), -8.0)\n"
     ]
    }
   ],
   "source": [
    "from autograd import grad\n",
    "import autograd.numpy as np\n",
    "import math\n",
    "from scipy.optimize import line_search\n",
    "NORM = np.linalg.norm\n",
    "\n",
    "def func(x): # Objective function\n",
    "    return 5*x[0]**2 + 6*x[0]*x[1] + 5*x[1]**2 - 8*math.sqrt(2)*x[0] - 8*math.sqrt(2)*x[1]\n",
    "\n",
    "Df = grad(func) # Gradient of the objective function\n",
    "\n",
    "def Hestenes_Stiefel(Xj, tol, alpha_1, alpha_2):\n",
    "    x1 = [Xj[0]]\n",
    "    x2 = [Xj[1]]\n",
    "    D = Df(Xj)\n",
    "    delta = -D\n",
    "    \n",
    "    while True:\n",
    "        start_point = Xj # Start point for step length selection \n",
    "        beta = line_search(f=func, myfprime=Df, xk=start_point, pk=delta, c1=alpha_1, c2=alpha_2)[0] # Selecting the step length\n",
    "        if beta!=None:\n",
    "            X = Xj+ beta*delta\n",
    "        \n",
    "        if NORM(Df(X)) < tol:\n",
    "            x1 += [X[0], ]\n",
    "            x2 += [X[1], ]\n",
    "            return X, func(X)\n",
    "        else:\n",
    "            Xj = X\n",
    "            d = D\n",
    "            D = Df(Xj)\n",
    "            chi = D.dot(D - d)/delta.dot(D - d) # See line (16)\n",
    "            delta = -D + chi*delta\n",
    "            x1 += [Xj[0], ]\n",
    "            x2 += [Xj[1], ]\n",
    "\n",
    "print(Hestenes_Stiefel(np.array([-1.7, -3.2]), 10**-6, 10**-4, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6a6bb3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.70710678, 0.70710678]), -8.000000000000002)\n"
     ]
    }
   ],
   "source": [
    "from autograd import grad\n",
    "import autograd.numpy as np\n",
    "import math\n",
    "from scipy.optimize import line_search\n",
    "NORM = np.linalg.norm\n",
    "\n",
    "def func(x): # Objective function\n",
    "    return 5*x[0]**2 + 6*x[0]*x[1] + 5*x[1]**2 - 8*math.sqrt(2)*x[0] - 8*math.sqrt(2)*x[1]\n",
    "\n",
    "Df = grad(func) # Gradient of the objective function\n",
    "\n",
    "def Dai_Yuan(Xj, tol, alpha_1, alpha_2):\n",
    "    x1 = [Xj[0]]\n",
    "    x2 = [Xj[1]]\n",
    "    D = Df(Xj)\n",
    "    delta = -D\n",
    "    \n",
    "    while True:\n",
    "        start_point = Xj # Start point for step length selection \n",
    "        beta = line_search(f=func, myfprime=Df, xk=start_point, pk=delta, c1=alpha_1, c2=alpha_2)[0] # Selecting the step length\n",
    "        if beta!=None:\n",
    "            X = Xj+ beta*delta\n",
    "        \n",
    "        if NORM(Df(X)) < tol:\n",
    "            x1 += [X[0], ]\n",
    "            x2 += [X[1], ]\n",
    "            return X, func(X)\n",
    "        else:\n",
    "            Xj = X\n",
    "            d = D\n",
    "            D = Df(Xj)\n",
    "            chi = NORM(D)**2/delta.dot(D - d) # See line (16)\n",
    "            delta = -D + chi*delta\n",
    "            x1 += [Xj[0], ]\n",
    "            x2 += [Xj[1], ]\n",
    "            \n",
    "print(Dai_Yuan(np.array([-1.7, -3.2]), 10**-6, 10**-4, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e8d80da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.70710678, 0.70710678]), -8.000000000000002)\n"
     ]
    }
   ],
   "source": [
    "from autograd import grad\n",
    "import autograd.numpy as np\n",
    "import math\n",
    "from scipy.optimize import line_search\n",
    "NORM = np.linalg.norm\n",
    "\n",
    "def func(x): # Objective function\n",
    "    return 5*x[0]**2 + 6*x[0]*x[1] + 5*x[1]**2 - 8*math.sqrt(2)*x[0] - 8*math.sqrt(2)*x[1]\n",
    "\n",
    "Df = grad(func) # Gradient of the objective function\n",
    "\n",
    "def Hager_Zhang(Xj, tol, alpha_1, alpha_2):\n",
    "    x1 = [Xj[0]]\n",
    "    x2 = [Xj[1]]\n",
    "    D = Df(Xj)\n",
    "    delta = -D\n",
    "    \n",
    "    while True:\n",
    "        start_point = Xj # Start point for step length selection \n",
    "        beta = line_search(f=func, myfprime=Df, xk=start_point, pk=delta, c1=alpha_1, c2=alpha_2)[0] # Selecting the step length\n",
    "        if beta!=None:\n",
    "            X = Xj+ beta*delta\n",
    "        \n",
    "        if NORM(Df(X)) < tol:\n",
    "            x1 += [X[0], ]\n",
    "            x2 += [X[1], ]\n",
    "            return X, func(X)\n",
    "        else:\n",
    "            Xj = X\n",
    "            d = D\n",
    "            D = Df(Xj)\n",
    "            Q = D - d\n",
    "            M = Q - 2*delta*NORM(Q)**2/(delta.dot(Q))\n",
    "            N = D/(delta.dot(Q))\n",
    "            chi = M.dot(N) # See line (19)\n",
    "            delta = -D + chi*delta\n",
    "            x1 += [Xj[0], ]\n",
    "            x2 += [Xj[1], ]\n",
    "            \n",
    "print(Hager_Zhang(np.array([-1.7, -3.2]), 10**-6, 10**-4, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fe6c9009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -8.000000\n",
      "         Iterations: 3\n",
      "         Function evaluations: 7\n",
      "         Gradient evaluations: 7\n",
      "[-9.  4.]\n",
      "[-4.20069713  5.57136627]\n",
      "[0.7676896  0.77699143]\n",
      "[0.70710678 0.70710678]\n"
     ]
    }
   ],
   "source": [
    "from autograd import grad\n",
    "import autograd.numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import math\n",
    "\n",
    "def func(x): # Objective function\n",
    "    return 5*x[0]**2 + 6*x[0]*x[1] + 5*x[1]**2 - 8*math.sqrt(2)*x[0] - 8*math.sqrt(2)*x[1]\n",
    "\n",
    "Df = grad(func) # Gradient of the objective function\n",
    "\n",
    "res=minimize(fun=func, x0=np.array([-9., 4.]), jac=Df, method='CG', options={'gtol':10**-6, 'disp':True, 'return_all':True})\n",
    "\n",
    "cnt = 1 # counter\n",
    "for i in res.allvecs: \n",
    "    print(i)\n",
    "    cnt = cnt + 1\n",
    "    # if cnt == 100:\n",
    "        # break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219cc5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
